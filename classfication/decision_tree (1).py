# -*- coding: utf-8 -*-
"""Decision_Tree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WMAaIhwtjM4eJXAxGOdblGbWx7fwwUUz
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df=pd.read_csv('winequality-red.csv')

df.info()

# df.isnull().sum()
df.head(5)

df['quality']=df['quality'].apply(lambda x:
                       0 if x<=4 else(1 if x<=6 else 2))

# df['quality'] = df['quality'].apply(
#     lambda x: 0 if x => 4 else (1 if x <= 6 else 2)
# )

# df['quality'].value_counts().sort_index()

df['quality'].unique()

for col in df.columns:
  plt.figure(figsize=(6,3))
  sns.boxplot(df[col])
  plt.show()

for col in df.columns:
  plt.figure(figsize=(6,3))
  sns.histplot(df[col],kde=True)
  plt.show()

independent=df.drop('quality',axis=1)
df['sulphates']=np.log1p(df['sulphates'])
df['total sulfur dioxide']=np.log1p(df['total sulfur dioxide'])

sns.histplot(df['chlorides'],kde=1)

for col in independent:
  q1,q3=df[col].quantile(0.25),df[col].quantile(0.75)
  iqr=q3-q1
  upper_bound=q3+1.5*iqr
  lower_bound=q1-1.5*iqr
  df[col]=df[col].apply(lambda x:
              upper_bound if x> upper_bound else(lower_bound if x < lower_bound else x )
              )

# for col in df.columns:
#   plt.figure(figsize=(6,3))
#   sns.boxplot(df[col])
#   plt.show()
df.corr()['quality'].sort_values(ascending=True)

df.head(5)

df.corr()['quality'].sort_values(ascending=True) *10
# result=df.corr()['quality'].sort_values(ascending=True)
# df.drop(['residual sugar', 'free sulfur dioxide', 'pH'],axis=1,inplace=True)

x,y=df.drop('quality',axis=1),df['quality']
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
# from sklearn.preprocessing import StandardScaler
# ss=StandardScaler()
# x_stand=ss.fit_transform(x)
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)
model=DecisionTreeClassifier()
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
confusion=confusion_matrix(y_test,y_pred)
disp=ConfusionMatrixDisplay(confusion_matrix=confusion)
disp.plot(cmap='viridis')
plt.show()

# from sklearn.metrics import classification_report
# print(classification_report(y_test, y_pred))
x,y=df.drop('quality',axis=1),df['quality']
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler,MinMaxScaler
ss=StandardScaler()
mm=MinMaxScaler(feature_range=(0,1))
x_stand=ss.fit_transform(x)

x_train,x_test,y_train,y_test=train_test_split(x_stand,y,test_size=0.3,random_state=1)
model = DecisionTreeClassifier(criterion='entropy',random_state=1)
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
# model.score(y_test,y_pred)
confusion=confusion_matrix(y_test,y_pred)
disp=ConfusionMatrixDisplay(confusion_matrix=confusion)
disp.plot(cmap='viridis')
plt.show()

importance=model.feature_importances_
importance=pd.Series(importance,index=x.columns)
importance